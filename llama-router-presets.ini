# Router Mode Presets for lemonade-sdk llama-server
# AMD Strix Halo (MAX+395, 128GB RAM, 126GB GTT iGPU)

# Some models are untested - submit a PR if you find an issue.

[DEFAULT]
# Global settings - optimized for Strix Halo
host = 0.0.0.0
port = 8080
ctx-size = 131072
batch-size = 8192
ubatch-size = 2048
temp = 1.0
top-p = 0.8
min-p = 0.01
n-gpu-layers = -1
parallel = 4
cont-batching = on
flash-attn = on
jinja = on
fit = on
sleep-idle-seconds = 300

[glm-4.7-flash]
hf-repo = unsloth/GLM-4.7-Flash-GGUF:Q8_0
alias = glm-4.7-flash
reasoning-format = deepseek
reasoning-budget = -1
ctx-size = 131072
# GLM-4 has native tool call support via chat template

[gpt-oss-120b]
hf-repo = unsloth/gpt-oss-120b-GGUF:Q4_K_M
alias = gpt-oss-120b
reasoning-format = deepseek
reasoning-budget = -1
ctx-size = 65536
# GPT-OSS has tool call support

# Braindead on strix halo...
[minimax-m2.1]
hf-repo = AaryanK/MiniMax-M2.1-GGUF:Q3_K_S
alias = minimax-m2.1
reasoning-format = deepseek
reasoning-budget = -1
ctx-size = 65536
cache-type-k = q8_0
cache-type-v = q8_0
parallel = 1
n-gpu-layers = 999

[mistral-large-2]
hf-repo = unsloth/Mistral-Large-Instruct-2407-GGUF:Q4_K_M
alias = mistral-large
ctx-size = 65536
# Optional: Enable cache quantization to save even more space
cache-type-k = q8_0
cache-type-v = q8_0

[llama-3.3-70b]
hf-repo = unsloth/Llama-3.3-70B-Instruct-GGUF:Q4_K_M
alias = llama-3.3
ctx-size = 131072
# You can likely afford full fp16 cache here, but q8_0 is safer
cache-type-k = q8_0
cache-type-v = q8_0

[qwen-3-coder-80b]
hf-repo = unsloth/Qwen3-Coder-Next-80B-A3B-Instruct-GGUF:Q4_K_M
alias = qwen-3-coder
# This MoE is incredibly fast (3B active params)
# 128k context is native for this model
ctx-size = 131072 
cache-type-k = q8_0
cache-type-v = q8_0
parallel = 1

[qwen-3-coder-32b]
hf-repo = unsloth/Qwen3-Coder-32B-Instruct-GGUF:Q4_K_M
alias = qwen-3-32b
ctx-size = 131072
cache-type-k = f16
cache-type-v = f16
parallel = 4

[gemma-3-27b]
hf-repo = unsloth/Gemma-3-27B-IT-GGUF:Q4_K_M
alias = gemma-3
# Native 128k context (huge upgrade over Gemma 2's 8k)
ctx-size = 131072
# You have so much RAM headroom with this model you can use high precision cache
cache-type-k = f16
cache-type-v = f16
# Run 4 concurrent requests easily
parallel = 4

[qwen-3-coder-agent]
# The specific "Next" version is critical for OpenClaw tool support
hf-repo = unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF:Q4_K_M
alias = openclaw-coder
# MoE models are fast; we can afford Q8 cache for precision
cache-type-k = q8_0
cache-type-v = q8_0
ctx-size = 131072
parallel = 1
# OpenClaw specific reliability setting
temperature = 0.1

[gemma-3-chat]
hf-repo = unsloth/Gemma-3-27B-IT-GGUF:Q4_K_M
alias = openclaw-chat
# Massive headroom allows full f16 cache for max quality
cache-type-k = f16
cache-type-v = f16
ctx-size = 131072
parallel = 4
