# Router Mode Presets for lemonade-sdk llama-server
# AMD Strix Halo (MAX+395, 128GB RAM, 126GB GTT iGPU)

# Some models are untested - submit a PR if you find an issue.

[DEFAULT]
# Global settings - optimized for Strix Halo
host = 0.0.0.0
port = 8080
ctx-size = 131072
batch-size = 8192
ubatch-size = 2048
temp = 1.0
top-p = 0.8
min-p = 0.01
n-gpu-layers = -1
parallel = 4
cont-batching = on
flash-attn = on
jinja = on
fit = on
sleep-idle-seconds = 300

# General + Coding
[glm-4.7-flash]
hf-repo = unsloth/GLM-4.7-Flash-GGUF:Q8_0
alias = glm-4.7-flash
reasoning-format = deepseek
reasoning-budget = -1
ctx-size = 131072

# General + Coding
[gpt-oss-120b]
hf-repo = unsloth/gpt-oss-120b-GGUF:Q4_K_M
alias = gpt-oss-120b
reasoning-format = deepseek
reasoning-budget = -1
ctx-size = 65536

# Coding + Agent
[mistral-large]
hf-repo = bartowski/Mistral-Large-Instruct-2411-GGUF:Q4_K_M
alias = mistral-large
ctx-size = 65536
cache-type-k = q8_0
cache-type-v = q8_0

# Coding + Agent
[llama-3.3]
hf-repo = unsloth/Llama-3.3-70B-Instruct-GGUF:Q4_K_XL
alias = llama-3.3
ctx-size = 131072
cache-type-k = q8_0
cache-type-v = q8_0

# Coding Agent
[qwen-3-coder]
hf-repo = unsloth/Qwen3-Coder-Next-GGUF:Q4_K_XL
alias = qwen-3-coder
ctx-size = 131072
cache-type-k = f16
cache-type-v = f16
parallel = 4

# Chat based only - doesn't work with opencode
[gemma-3]
hf-repo = unsloth/Qwen3-32B-GGUF:Q8_K_XL
alias = gemma-3
ctx-size = 131072
cache-type-k = f16
cache-type-v = f16
parallel = 4
